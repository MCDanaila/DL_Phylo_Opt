{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Align import AlignInfo\n",
    "from Bio import AlignIO\n",
    "\n",
    "\n",
    "from Tools import PhyML, RaxML\n",
    "from Utils import tree_functions\n",
    "from Utils.defs_PhyAI import ROOTLIKE_NAME, TEST_DATA_PATH, SEP, DEFAULT_MODEL, SUBTREE1, SUBTREE2, LEARNING_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_input(msa_file, user_tree_file):\n",
    "\t\"\"\"\n",
    "\t:param msa_file: the path to an MSA file, one of biopython's formats\n",
    "\t:param user_tree_file: (optional) the path to a user tree file, if fixed tree was desired\n",
    "\t:return: a biopython object of the msa and an ete3 object of the tree if exists\n",
    "\t\"\"\"\n",
    "\t# identify format and retrieve all MSAs\n",
    "\tfor aln_format in [\"clustal\", \"emboss\", \"fasta\", \"fasta-m10\", \"ig\", \"maf\", \"mauve\", \"nexus\", \"phylip-relaxed\", \"phylip-sequential\", \"stockholm\"]:\n",
    "\t\ttry:\n",
    "\t\t\tmsa_obj = AlignIO.read(msa_file, format=aln_format)\n",
    "\t\t\tprint(\"INFO - The MSA file is format: \" + aln_format)\n",
    "\t\t\tbreak\n",
    "\t\texcept Exception:\n",
    "\t\t\tmsa_obj = None\n",
    "\tif msa_obj is None:\n",
    "\t\tprint(\"ERROR - Error occured: the input file is not a valid alignmnet in a supported format.\\n\"\n",
    "\t\t\t\t\t \"Please verify that all sequences are at the same length and that the input format is correct.\")\n",
    "\t# validate MSA characters\n",
    "\tmsa_info = AlignInfo.SummaryInfo(msa_obj)\n",
    "\taln_letters = msa_info._get_all_letters()\n",
    "\tfor let in aln_letters:\n",
    "\t\tif not (let.lower() in \"acgt-\"):\n",
    "\t\t\tprint(\"WARNING - There are characters that are not nucleotides or gaps in your input MSA.\")\n",
    "\t\t\tbreak\n",
    "\t# validate tree file in Newick format and suits the msa\n",
    "\ttree_obj = None\n",
    "\tif user_tree_file:\n",
    "\t\ttry:\n",
    "\t\t\twith open(user_tree_file) as fpr:\n",
    "\t\t\t\ttree_obj = tree_functions.get_newick_tree(fpr.read().strip())\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"ERROR - Tree file is invalid. Please verify that it's in Newick format.\")\n",
    "\t\tprint(tree_obj)\n",
    "\t\t# assert that the tree matches the corresponding MSA\n",
    "\t\tleaves = sorted([node.name for node in tree_obj.get_leaves()])\n",
    "\t\tseq_names = sorted([rec.id for rec in msa_obj])\n",
    "\t\tif len(leaves) != len(seq_names) or (not all(x == y for x,y  in zip(seq_names,leaves))):\n",
    "\t\t\tprint(\"ERROR - The tips of the tree and the MSA sequences names do not match\")\n",
    "\n",
    "\treturn msa_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for folder_name in os.listdir(TEST_DATA_PATH):\n",
    "folder_name = '82'\n",
    "CWD = TEST_DATA_PATH + folder_name + SEP\n",
    "orig_msa_file = CWD + 'real_msa.phy'\n",
    "run_id = DEFAULT_MODEL + \"_\" + folder_name\n",
    "stats_file, tree_file = PhyML.run_phyml(orig_msa_file, DEFAULT_MODEL, run_id=run_id)\n",
    "#msa_obj = validate_input(orig_msa_file, tree_file)\n",
    "\n",
    "orig_tree_obj = tree_functions.get_phylo_tree(tree_file, orig_msa_file)\n",
    "orig_tree_obj.get_tree_root().name = ROOTLIKE_NAME\n",
    "print(orig_tree_obj.get_ascii(attributes=[\"name\", \"dist\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "OUTPUT_TREES_FILE = CWD +  'newicks_step.csv'\n",
    "with open(OUTPUT_TREES_FILE, \"w\", newline='') as fpw:\n",
    "\tcsvwriter = csv.writer(fpw)\n",
    "\tcsvwriter.writerow(['iteration', 'prune_name', 'rgft_name', 'newick'])\n",
    "print(\"RUN: parse_phyml_stats_output ===========\")\n",
    "params_dict = (PhyML.parse_phyml_stats_file(stats_file))\n",
    "#keep pinv and alpha \n",
    "freq, rates, pinv, alpha = [params_dict[\"fA\"], params_dict[\"fC\"], params_dict[\"fG\"], params_dict[\"fT\"]], [params_dict[\"subAC\"], params_dict[\"subAG\"], params_dict[\"subAT\"], params_dict[\"subCG\"],params_dict[\"subCT\"], params_dict[\"subGT\"]], params_dict[\"pInv\"], params_dict[\"gamma\"]\n",
    "df = pd.DataFrame()\n",
    "orig_ds_ll = float(params_dict[\"logL\"])\n",
    "root_children = orig_tree_obj.get_tree_root().get_children()\n",
    "outpath_prune = CWD + 'ds_summary_SPR_prune.csv'\n",
    "outpath_rgft = CWD + 'ds_summary_SPR_rgft.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prune_node in enumerate(orig_tree_obj.iter_descendants(\"levelorder\")):\n",
    "\tif prune_node in root_children:\n",
    "\t\tprint(f\"> SKIPPED Iteration {i} with pruned tree at node name: {prune_node.get_tree_root().name}\")\n",
    "\t\tcontinue\n",
    "\tprune_name = prune_node.name\n",
    "\t#x = prune_node.get_ascii(attributes=[\"name\", \"dist\"])\n",
    "\t#print(f\"+++++++++ Iteration {i} with pruned tree: {x} \\n At node name: {prune_name} \\n+++++++++\")\n",
    "\tnname, subtree1, subtree2 = tree_functions.prune_branch(orig_tree_obj, prune_name) # subtree1 is the pruned subtree. subtree2 is the remaining subtree\n",
    "\tprint(f\"+++++++++ Iteration {i} with PRUNED TREE at node name: {nname}\")\n",
    "\t#print(\">> pruned node: \", nname)\n",
    "\t#print(\">> pruned subtree1\", subtree1.get_ascii(attributes=[\"name\", \"dist\"]))\n",
    "\t#print(f\">> ROOT: {subtree1.get_tree_root().name}\")\n",
    "\t#print(\">> remaining subtree2\", subtree2.get_ascii(attributes=[\"name\", \"dist\"]))\n",
    "\t#print(f\">> ROOT: {subtree2.get_tree_root().name}\")\n",
    "\t#print(\"================\")\n",
    "\twith open(OUTPUT_TREES_FILE, \"a\", newline='') as fpa:\n",
    "\t\tcsvwriter = csv.writer(fpa)\n",
    "\t\tcsvwriter.writerow([str(i)+\",0\", prune_name, SUBTREE1, subtree1.write(format=1)])\n",
    "\t\tcsvwriter.writerow([str(i)+\",1\", prune_name, SUBTREE2, subtree2.write(format=1)])\n",
    "\n",
    "\tfor j, rgft_node in enumerate(subtree2.iter_descendants(\"levelorder\")): # traversing over subtree2 capture cases (1) and (3)\n",
    "\t\t# skip the ROOT node when regraft \n",
    "\t\tind = str(i) + \",\" + str(j)\n",
    "\t\trgft_name = rgft_node.name\n",
    "\t\t#y = rgft_node.get_ascii(attributes=[\"name\", \"dist\"])\n",
    "\t\t#print(f\"++++++++++++++++++ Iteration {ind} with remaining tree: {y} \\n At node name: {rgft_name} \\n++++++++++++++++++\")\n",
    "\t\tif nname == rgft_name: # captures case (2)\n",
    "\t\t\tcontinue\n",
    "\t\tprint(f\"++++++++++++++++++ Iteration {ind} REGRAFT TREE at node name: {rgft_name}\")\n",
    "\t\trearr_tree, preserve = tree_functions.regraft_branch(subtree2, rgft_node, subtree1, rgft_name, nname)\n",
    "\t\t\n",
    "\t\t#print(\">> rearr_tree: \", rearr_tree.get_ascii(attributes=[\"name\", \"dist\"]))\n",
    "\t\t#print(\"-- rearr_tree has ROOT: \", rearr_tree.get_tree_root().name)\n",
    "\t\tneighbor_tree_str = rearr_tree.write(format=1, format_root_node=True)\n",
    "\n",
    "\t\t### save tree to file by using \"append\"\n",
    "\t\twith open(OUTPUT_TREES_FILE, \"a\", newline='') as fpa:\n",
    "\t\t\tcsvwriter = csv.writer(fpa)\n",
    "\t\t\tcsvwriter.writerow([ind, prune_name, rgft_name, neighbor_tree_str])\n",
    "\t\t#print(\"====== neighbor_tree_str ==========\")\n",
    "\t\t#print(neighbor_tree_str)\n",
    "\t\t#neighbor_tree_str = neighbor_tree_str.replace(\";\", \"R:0.0;\")\n",
    "\t\t#total_bl = tree_functions.get_total_branch_lengths(neighbor_tree_str)\n",
    "\t\tll_rearr, rtime = RaxML.call_raxml_mem(neighbor_tree_str, orig_msa_file, rates, pinv, alpha, freq)\n",
    "\t\tprint(f\"INFO - Total branch lenght: {tree_functions.get_total_branch_lengths(neighbor_tree_str)}\")\n",
    "\t\t#print(f\"INFO - Total branch lenght: {total_bl}\")\n",
    "\t\t#print(f\"INFO - Rearranged Likelihood: {ll_rearr}\" )\n",
    "\t\t\n",
    "\t\tdf[\"orig_ds_ll\"] = orig_ds_ll\n",
    "\t\tdf.loc[ind, \"prune_name\"], df.loc[ind, \"rgft_name\"] = prune_name, rgft_name\n",
    "\t\tdf.loc[ind, \"time\"] = rtime\n",
    "\t\tdf.loc[ind, \"ll\"] = ll_rearr\n",
    "\n",
    "df.to_csv(outpath_prune.format(\"prune\"), index_label='iteration')\n",
    "df.to_csv(outpath_rgft.format(\"rgft\"), index_label='iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import collect_features\n",
    "\n",
    "collect_features.collect_features(CWD, tree_file, outpath_prune, outpath_rgft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Utils.defs_PhyAI import DATA_WITH_PREDS, FEATURES_SHARED, FEATURES, LABEL, SCORES_PER_DS, types_dict, FEATURES_RGFT_ONLY, FEATURES_MERGED, FEATURES_RGFT, FEATURES_PRUNE, GROUP_ID\n",
    "from Utils import RF_learning_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Utils.tree_functions import get_total_branch_lengths\n",
    "from Utils.defs_PhyAI import PHYML_TREE_FILENAME, SUMMARY_PER_DS\n",
    "\n",
    "def parse_relevant_summaries_for_learning(datapath, outpath, move_type, step_number, tree_type='bionj'):\n",
    "\tcols = FEATURES_PRUNE if move_type == \"prune\" else FEATURES_RGFT + [\"prune_name\",\"rgft_name\",\"orig_ds_ll\", \"ll\", \"time\", \"d_ll_prune\"]\n",
    "\tdf = pd.DataFrame(columns=cols)\n",
    "\tfor i,relpath in enumerate(os.listdir(datapath)):\n",
    "\t\tif os.path.isdir(os.path.join(datapath, relpath)):\n",
    "\t\t\tprint(i, relpath)\n",
    "\n",
    "\t\t\tds_path = datapath + relpath + '/'\n",
    "\t\t\tuser_tree_file = os.path.join(ds_path, tree_file.format(relpath))\n",
    "\t\t\twith open(user_tree_file) as fpr:\n",
    "\t\t\t\tds_tbl = get_total_branch_lengths(fpr.read().strip())\n",
    "\t\t\tsummary_per_ds = SUMMARY_PER_DS.format(ds_path, move_type)\n",
    "\n",
    "\t\t\tif os.path.exists(summary_per_ds) and FEATURES[\"bl\"] in pd.read_csv(summary_per_ds).columns:\n",
    "\t\t\t\tdf_ds = pd.read_csv(summary_per_ds, index_col='iteration')\n",
    "\t\t\t\t#df_ds.insert(1, \"path\", datapath)\n",
    "\t\t\t\tdf_ds[FEATURES[GROUP_ID]] = str(i)\n",
    "\t\t\t\tdf_ds[FEATURES[\"group_tbl\"]] = ds_tbl\n",
    "\t\t\t\t#print(df_ds)\n",
    "\t\t\t\tdf = pd.concat([df, df_ds], join='inner', axis=0)\n",
    "\t\t\tdf.to_csv(outpath, index_label='iteration')\n",
    "\t\t\t#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "move_type, st = \"merged\", \"1\"\n",
    "transform_target, validation_set = False, False\n",
    "#print(CWD)\n",
    "df_path = TEST_DATA_PATH + LEARNING_DATA.format(\"all_moves\")\n",
    "#print(df_path)\n",
    "df_prune_features = TEST_DATA_PATH + LEARNING_DATA.format(\"all_moves_prune\")\n",
    "#print(df_prune_features)\n",
    "df_rgft_features = TEST_DATA_PATH + LEARNING_DATA.format(\"all_moves_rgft\")\n",
    "#print(df_rgft_features)\n",
    "\n",
    "\n",
    "parse_relevant_summaries_for_learning(TEST_DATA_PATH, df_prune_features, \"prune\", st)\n",
    "parse_relevant_summaries_for_learning(TEST_DATA_PATH, df_rgft_features, \"rgft\", st)\n",
    "\n",
    "#complete_df = pd.read_csv(df_prune_features, dtype=types_dict).merge(pd.read_csv(df_rgft_features, dtype=types_dict),on=shared_cols, left_index=True, right_index=True, suffixes=('_prune', '_rgft'))\n",
    "\n",
    "shared_cols = FEATURES_SHARED + [\"iteration\", \"prune_name\",\"rgft_name\",\"orig_ds_ll\"]\n",
    "\n",
    "df_prune_moves = pd.read_csv(df_prune_features, dtype=types_dict, index_col=0)\n",
    "print(df_prune_moves.columns)\n",
    "\n",
    "df_rgft_moves = pd.read_csv(df_rgft_features, dtype=types_dict, index_col=0)\n",
    "print(df_rgft_moves.columns)\n",
    "\n",
    "complete_df = pd.merge(df_prune_moves, df_rgft_moves, how='outer', on=shared_cols, suffixes=('_prune', '_rgft'))\n",
    "\n",
    "complete_df = complete_df.rename(columns={FEATURES[f]: FEATURES[f] + \"_rgft\" for f in FEATURES_RGFT_ONLY})\n",
    "complete_df[LABEL.format(move_type)] = complete_df[LABEL.format(\"prune\")]\n",
    "complete_df.to_csv(df_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1s/5_jwsmtn2n54h333qvdtr32c0000gp/T/ipykernel_88088/472130274.py:11: DtypeWarning: Columns (5,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_learning = pd.read_csv(df_path, dtype=types_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*@*@*@* scores for step_1 with 20 features are not available, thus applying learning\n",
      "10 0 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n",
      " 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n",
      " 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460] 460 46\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n",
      " 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n",
      " 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460] 46\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['topology_dist_between_rgft', 'tbl_dist_between_rgft', 'res_tree_edge_length_rgft'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/mihaid/Coding-Projects/thesis/DL_Phylo_Opt/sequence.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihaid/Coding-Projects/thesis/DL_Phylo_Opt/sequence.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*@*@*@* scores for step\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m features are not available, thus applying learning\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(suf, \u001b[39mlen\u001b[39m(features)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihaid/Coding-Projects/thesis/DL_Phylo_Opt/sequence.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#print(\"BEFORE RF: \", df_learning)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mihaid/Coding-Projects/thesis/DL_Phylo_Opt/sequence.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m res_dict, df_out \u001b[39m=\u001b[39m RF_learning_algorithm\u001b[39m.\u001b[39mcross_validation_RF(df_learning, move_type, features, trans\u001b[39m=\u001b[39mtransform_target ,validation_set\u001b[39m=\u001b[39mvalidation_set)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihaid/Coding-Projects/thesis/DL_Phylo_Opt/sequence.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m df_out\u001b[39m.\u001b[39mto_csv(csv_with_preds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihaid/Coding-Projects/thesis/DL_Phylo_Opt/sequence.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m df_datasets \u001b[39m=\u001b[39m  pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Coding-Projects/thesis/DL_Phylo_Opt/Utils/RF_learning_algorithm.py:115\u001b[0m, in \u001b[0;36mcross_validation_RF\u001b[0;34m(df, move_type, features, trans, validation_set, random, scale_score)\u001b[0m\n\u001b[1;32m    112\u001b[0m df_test \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[df[FEATURES[GROUP_ID]]\u001b[39m.\u001b[39misin(test_ixs)]\n\u001b[1;32m    113\u001b[0m df_train \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[df[FEATURES[GROUP_ID]]\u001b[39m.\u001b[39misin(train_ixs)]\n\u001b[0;32m--> 115\u001b[0m y_pred, oob, f_imp \u001b[39m=\u001b[39m apply_RFR(df_test, df_train, move_type, features)\n\u001b[1;32m    117\u001b[0m oobs\u001b[39m.\u001b[39mappend(oob)\n\u001b[1;32m    118\u001b[0m f_imps\u001b[39m.\u001b[39mappend(f_imp)\n",
      "File \u001b[0;32m~/Coding-Projects/thesis/DL_Phylo_Opt/Utils/RF_learning_algorithm.py:67\u001b[0m, in \u001b[0;36mapply_RFR\u001b[0;34m(df_test, df_train, move_type, features)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_RFR\u001b[39m(df_test, df_train, move_type, features):\n\u001b[0;32m---> 67\u001b[0m \tX_train, y_train \u001b[39m=\u001b[39m split_features_label(df_train, move_type, features)\n\u001b[1;32m     68\u001b[0m \tX_test, y_test \u001b[39m=\u001b[39m split_features_label(df_test, move_type, features)\n\u001b[1;32m     70\u001b[0m \tregressor \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39mN_ESTIMATORS, max_features\u001b[39m=\u001b[39m\u001b[39m0.33\u001b[39m,  oob_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mfit(X_train, y_train) \u001b[39m# 0.33=nfeatures/3. this is like in R (instead of default=n_features)\u001b[39;00m\n",
      "File \u001b[0;32m~/Coding-Projects/thesis/DL_Phylo_Opt/Utils/RF_learning_algorithm.py:57\u001b[0m, in \u001b[0;36msplit_features_label\u001b[0;34m(df, move_type, features)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_features_label\u001b[39m(df, move_type, features):\n\u001b[0;32m---> 57\u001b[0m \tattributes_df \u001b[39m=\u001b[39m df[features]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \tlabel_df \u001b[39m=\u001b[39m df[LABEL\u001b[39m.\u001b[39mformat(move_type)]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     60\u001b[0m \tx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(attributes_df)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/phytree/lib/python3.11/site-packages/pandas/core/frame.py:3902\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3900\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3901\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3902\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39m_get_indexer_strict(key, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3904\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/phytree/lib/python3.11/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/phytree/lib/python3.11/site-packages/pandas/core/indexes/base.py:6178\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6175\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 6178\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['topology_dist_between_rgft', 'tbl_dist_between_rgft', 'res_tree_edge_length_rgft'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from Utils.defs_PhyAI import DATA_WITH_PREDS, FEATURES_SHARED, FEATURES, LABEL, SCORES_PER_DS, types_dict, FEATURES_RGFT_ONLY, FEATURES_MERGED, FEATURES_RGFT, FEATURES_PRUNE, GROUP_ID\n",
    "from Utils import RF_learning_algorithm\n",
    "\n",
    "move_type, st = \"merged\", \"1\"\n",
    "transform_target, validation_set = False, False\n",
    "CWD = \"/Users/mihaid/Coding-Projects/thesis/DL_Phylo_Opt/data/training_data/\"\n",
    "df_path = CWD + \"learning_all_moves.csv\"\n",
    "df_learning = pd.read_csv(df_path, dtype=types_dict)\n",
    "#print(\"BEFORE: \", df_learning)\n",
    "#df_learning['orig_ds_id'] = df_learning['orig_ds_id_rgft']\n",
    "df_learning = RF_learning_algorithm.fit_transformation(df_learning, move_type, trans=transform_target)\n",
    "#print(\"AFTER_FIT: \", df_learning)\n",
    "features = FEATURES_PRUNE if move_type == \"prune\" else FEATURES_RGFT if move_type == \"rgft\" else FEATURES_MERGED\n",
    "if FEATURES[GROUP_ID] in features:\n",
    "\t#print(features)\n",
    "\tfeatures.remove(FEATURES[GROUP_ID])\n",
    "\n",
    "########################\n",
    "\n",
    "suf = \"_{}_validation_set\".format(st) if validation_set else \"_{}\".format(st)\n",
    "iftrans = \"\" if not transform_target else \"_ytransformed\"\n",
    "suf += iftrans\n",
    "csv_with_scores = CWD + SCORES_PER_DS.format(str(len(features))+ suf)\n",
    "csv_with_preds = CWD + DATA_WITH_PREDS.format(str(len(features)) + suf)\n",
    "if not os.path.exists(csv_with_scores) or validation_set:\n",
    "\tprint(\"*@*@*@* scores for step{} with {} features are not available, thus applying learning\".format(suf, len(features)))\n",
    "\t#print(\"BEFORE RF: \", df_learning)\n",
    "\tres_dict, df_out = RF_learning_algorithm.cross_validation_RF(df_learning, move_type, features, trans=transform_target ,validation_set=validation_set)\n",
    "\tdf_out.to_csv(csv_with_preds)\n",
    "\n",
    "\tdf_datasets =  pd.DataFrame(columns=[\"init\"])\n",
    "else:\n",
    "\tdf_datasets = pd.read_csv(csv_with_scores)\n",
    "\tres_dict = RF_learning_algorithm.extract_scores_dict({}, df_datasets)\n",
    "df_datasets = RF_learning_algorithm.print_and_index_results(df_datasets, res_dict, features)\n",
    "df_datasets.to_csv(csv_with_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phytree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
